{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from bs4 import BeautifulSoup\r\n",
    "\r\n",
    "html = r'X:\\Misinfo\\HTML\\real\\3.html'\r\n",
    "\r\n",
    "with open(html, encoding='utf8') as f:\r\n",
    "    soup = BeautifulSoup(f, 'html.parser')\r\n",
    "f.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Corresponding to all_meta_combined.ipynb\r\n",
    "\r\n",
    "Create dictionary of site, label, and all meta tags combined"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from bs4 import BeautifulSoup\r\n",
    "import pandas as pd\r\n",
    "import os\r\n",
    "from pathlib import Path\r\n",
    "import numpy as np\r\n",
    "import pickle\r\n",
    "from tqdm import tqdm\r\n",
    "\r\n",
    "# Get url keys\r\n",
    "with open('real_url_key.pkl', 'rb') as f:\r\n",
    "    real_url_key = pickle.load(f)\r\n",
    "f.close()\r\n",
    "with open('fake_url_key.pkl', 'rb') as f:\r\n",
    "    fake_url_key = pickle.load(f)\r\n",
    "f.close()\r\n",
    "\r\n",
    "directory1 = Path.cwd() / 'HTML' / 'real'\r\n",
    "directory2 = Path.cwd() / 'HTML' / 'fake'\r\n",
    "directory = [directory1, directory2]\r\n",
    "\r\n",
    "dict_ = {'index' : [],\r\n",
    "        'label' : [],\r\n",
    "        'site' : [],\r\n",
    "        'meta_content_combined' : []}\r\n",
    "\r\n",
    "# The first loop creates the name list, the second loop creates vectors for each site corresponding to the number of each\r\n",
    "num_without_content = 0\r\n",
    "for dir_ in tqdm(directory):\r\n",
    "    for html in os.listdir(dir_):\r\n",
    "        # Open html file as soup object\r\n",
    "        with open(str(dir_) + '/' + html, encoding='utf8') as f:\r\n",
    "            soup = BeautifulSoup(f, 'html.parser')\r\n",
    "        f.close()\r\n",
    "\r\n",
    "        # Populate dict\r\n",
    "        index = int(str(html).split('.')[0])\r\n",
    "        label = str(dir_)[-4:]\r\n",
    "        site = real_url_key[index] if str(dir_)[-4:] == 'real' else fake_url_key[index]\r\n",
    "\r\n",
    "        metatags = soup.find_all('meta')\r\n",
    "        content = ''\r\n",
    "        for tag in metatags:\r\n",
    "            try:\r\n",
    "                content += ' ' + tag.attrs['content']\r\n",
    "            except:\r\n",
    "                try:\r\n",
    "                    content += ' ' + tag.attrs['value']\r\n",
    "                except:\r\n",
    "                    continue\r\n",
    "            \r\n",
    "            \r\n",
    "        \r\n",
    "        dict_['index'].append(index)\r\n",
    "        dict_['label'].append(label)\r\n",
    "        dict_['site'].append(site)\r\n",
    "        dict_['meta_content_combined'].append(content)\r\n",
    "\r\n",
    "print('\\nThere are {} sites without meta content'.format(num_without_content))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "save to csv"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "df = pd.DataFrame(dict_)\r\n",
    "df_csv = pd.read_csv('html_data_expanded.csv')\r\n",
    "joined = df_csv.merge(df, on=['index', 'label'])\r\n",
    "joined.rename(columns={'Column1' : 'global_index', 'site_x' : 'site'}, inplace=True)\r\n",
    "joined.drop(columns=['site_y'], inplace=True)\r\n",
    "joined.to_csv('all_meta_combined.csv')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "' text/html; charset=UTF-8 width=device-width,initial-scale=1 A+b/H0b8RPXNaJgaNFpO0YOFuGK6myDQXlwnJB3SwzvNMfcndat4DZYMrP4ClJIzYWo3/yP2S+8FTZ/lpqbPAAEAAABueyJvcmlnaW4iOiJodHRwczovL2ltYXNkay5nb29nbGVhcGlzLmNvbTo0NDMiLCJmZWF0dXJlIjoiVHJ1c3RUb2tlbnMiLCJleHBpcnkiOjE2MjYyMjA3OTksImlzVGhpcmRQYXJ0eSI6dHJ1ZX0= A9ZgbRtm4pU3oZiuNzOsKcC8ppFSZdcjP2qYcdQrFKVzkmiWH1kdYY1Mi9x7G8+PS8HV9Ha9Cz0gaMdKsiVZIgMAAAB7eyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiVHJ1c3RUb2tlbnMiLCJleHBpcnkiOjE2MjYyMjA3OTksImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9 AxL6oBxcpn5rQDPKSAs+d0oxNyJYq2/4esBUh3Yx5z8QfcLu+AU8iFCXYRcr/CEEfDnkxxLTsvXPJFQBxHfvkgMAAACBeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXRhZ3NlcnZpY2VzLmNvbTo0NDMiLCJmZWF0dXJlIjoiVHJ1c3RUb2tlbnMiLCJleHBpcnkiOjE2MjYyMjA3OTksImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9 A9KPtG5kl3oLTk21xqynDPGQ5t18bSOpwt0w6kGa6dEWbuwjpffmdUpR3W+faZDubGT+KIk2do0BX2ca16x8qAcAAACBeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiVHJ1c3RUb2tlbnMiLCJleHBpcnkiOjE2MjYyMjA3OTksImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9 AookgM0K6zABiuRTZwpn+R95G2CKmUH/2+zf2kS/QpMlVZ6HTI6QekeLkrJyxeIi62p2ejcQTF464pkdlx0Nwg0AAABmeyJvcmlnaW4iOiJodHRwczovL3d3dy5nb29nbGUuY29tOjQ0MyIsImZlYXR1cmUiOiJUcnVzdFRva2VucyIsImV4cGlyeSI6MTYzNDA4MzE5OSwiaXNTdWJkb21haW4iOnRydWV9 A3HucHUo1oW9s+9kIKz8mLkbcmdaj5lxt3eiIMp1Nh49dkkBlg1Fhg4Fd/r0vL69mRRA36YutI9P/lJUfL8csQoAAACFeyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiQ29udmVyc2lvbk1lYXN1cmVtZW50IiwiZXhwaXJ5IjoxNjI2MjIwNzk5LCJpc1N1YmRvbWFpbiI6dHJ1ZSwiaXNUaGlyZFBhcnR5Ijp0cnVlfQ== A0OysezhLoCRYomumeYlubLurZTCmsjTb087OvtCy95jNM65cfEsbajrJnhaGwiTxhz38ZZbm+UhUwQuXfVPTg0AAACLeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiQ29udmVyc2lvbk1lYXN1cmVtZW50IiwiZXhwaXJ5IjoxNjI2MjIwNzk5LCJpc1N1YmRvbWFpbiI6dHJ1ZSwiaXNUaGlyZFBhcnR5Ijp0cnVlfQ== AxoOxdZQmIoA1WeAPDixRAeWDdgs7ZtVFfH2y19ziTgD1iaHE5ZGz2UdSjubkWvob9C5PrjUfkWi4ZSLgWk3Xg8AAACLeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXRhZ3NlcnZpY2VzLmNvbTo0NDMiLCJmZWF0dXJlIjoiQ29udmVyc2lvbk1lYXN1cmVtZW50IiwiZXhwaXJ5IjoxNjI2MjIwNzk5LCJpc1N1YmRvbWFpbiI6dHJ1ZSwiaXNUaGlyZFBhcnR5Ijp0cnVlfQ== A7+rMYR5onPnACrz+niKSeFdH3xw1IyHo2AZSHmxrofRk9w4HcQPMYcpBUKu6OQ6zsdxf4m/vqa6tG6Na4OLpAQAAAB4eyJvcmlnaW4iOiJodHRwczovL2ltYXNkay5nb29nbGVhcGlzLmNvbTo0NDMiLCJmZWF0dXJlIjoiQ29udmVyc2lvbk1lYXN1cmVtZW50IiwiZXhwaXJ5IjoxNjI2MjIwNzk5LCJpc1RoaXJkUGFydHkiOnRydWV9 AwfG8hAcHnPa/kJ1Co0EvG/K0F9l1s2JZGiDLt2mhC3QI5Fh4qmsmSwrWObZFbRC9ieDaSLU6lHRxhGUF/i9sgoAAACBeyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiSW50ZXJlc3RDb2hvcnRBUEkiLCJleHBpcnkiOjE2MjYyMjA3OTksImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9 AwQ7dCmHkvR6FuOFxAuNnktYSQrGbL4dF+eBkrwNLALc69Wr//PnO1yzns3pjUoCaYbKHtVcnng2hU+8OUm0PAYAAACHeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiSW50ZXJlc3RDb2hvcnRBUEkiLCJleHBpcnkiOjE2MjYyMjA3OTksImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9 AysVDPGQTLD/Scn78x4mLwB1tMfje5jwUpAAzGRpWsr1NzoN7MTFhT3ClmImi2svDZA7V6nWGIV8YTPsSRTe0wYAAACHeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXRhZ3NlcnZpY2VzLmNvbTo0NDMiLCJmZWF0dXJlIjoiSW50ZXJlc3RDb2hvcnRBUEkiLCJleHBpcnkiOjE2MjYyMjA3OTksImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9 WordPress 5.1.1 Sarugbymag 4 Latest rugby news now · videos, fixtures, results, opinion, logs · Springboks, All Blacks, Wallabies and Championship, Tournaments, Super Rugby updates en_US website SA Rugbymag | Latest Rugby News, Opinion, Fixtures, Results Now Latest rugby news now · videos, fixtures, results, opinion, logs · Springboks, All Blacks, Wallabies and Championship, Tournaments, Super Rugby updates https://www.sarugbymag.co.za/ SA Rugbymag summary Latest rugby news now · videos, fixtures, results, opinion, logs · Springboks, All Blacks, Wallabies and Championship, Tournaments, Super Rugby updates SA Rugbymag | Latest Rugby News, Opinion, Fixtures, Results Now @SARugbymag'"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## To get list of different meta tags"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from bs4 import BeautifulSoup\r\n",
    "import pandas as pd\r\n",
    "import os\r\n",
    "from pathlib import Path\r\n",
    "import numpy as np\r\n",
    "import pickle\r\n",
    "from tqdm import tqdm\r\n",
    "\r\n",
    "# Get url keys\r\n",
    "with open('real_url_key.pkl', 'rb') as f:\r\n",
    "    real_url_key = pickle.load(f)\r\n",
    "f.close()\r\n",
    "with open('fake_url_key.pkl', 'rb') as f:\r\n",
    "    fake_url_key = pickle.load(f)\r\n",
    "f.close()\r\n",
    "\r\n",
    "directory1 = Path.cwd() / 'HTML' / 'real'\r\n",
    "directory2 = Path.cwd() / 'HTML' / 'fake'\r\n",
    "directory = [directory1, directory2]\r\n",
    "# Will hold all the potential meta \"name\" categories\r\n",
    "# non case sensitive\r\n",
    "name_list = []\r\n",
    "\r\n",
    "dict_ = {'index' : [],\r\n",
    "        'label' : [],\r\n",
    "        'site' : [],\r\n",
    "        'meta_names' : []}\r\n",
    "\r\n",
    "# The first loop creates the name list, the second loop creates vectors for each site corresponding to the number of each\r\n",
    "for dir_ in tqdm(directory):\r\n",
    "    for html in os.listdir(dir_):\r\n",
    "        # Open html file as soup object\r\n",
    "        with open(str(dir_) + '/' + html, encoding='utf8') as f:\r\n",
    "            soup = BeautifulSoup(f, 'html.parser')\r\n",
    "        f.close()\r\n",
    "\r\n",
    "        metatags = soup.find_all('meta')\r\n",
    "        for tag in metatags:\r\n",
    "            try:\r\n",
    "                name_list.append(tag.attrs['name'])\r\n",
    "            except:\r\n",
    "                continue\r\n",
    "\r\n",
    "name_list = list(set(name_list))\r\n",
    "\r\n",
    "meta_name_vocab = {}\r\n",
    "for i in range(len(name_list)):\r\n",
    "    meta_name_vocab[i] = name_list[i]\r\n",
    "\r\n",
    "with open('meta_name_vocab.pkl', 'wb') as f:\r\n",
    "    pickle.dump(meta_name_vocab, f)\r\n",
    "f.close()\r\n",
    "\r\n",
    "\r\n",
    "# df = pd.DataFrame(dict_)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 2/2 [04:18<00:00, 129.40s/it]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "dict_ = {'index' : [],\r\n",
    "        'label' : [],\r\n",
    "        'site' : [],\r\n",
    "        'name_vector' : []}\r\n",
    "\r\n",
    "meta_name_dict_default = {}\r\n",
    "for name in name_list:\r\n",
    "    meta_name_dict_default[name] = 0\r\n",
    "\r\n",
    "for dir_ in tqdm(directory):\r\n",
    "    for html in os.listdir(dir_):\r\n",
    "        # Construct Dictionary\r\n",
    "        meta_name_dict = meta_name_dict_default.copy()\r\n",
    "\r\n",
    "        # Open html file as soup object\r\n",
    "        with open(str(dir_) + '/' + html, encoding='utf8') as f:\r\n",
    "            soup = BeautifulSoup(f, 'html.parser')\r\n",
    "        f.close()\r\n",
    "\r\n",
    "        index = int(str(html).split('.')[0])\r\n",
    "        label = str(dir_)[-4:]\r\n",
    "        site = real_url_key[index] if str(dir_)[-4:] == 'real' else fake_url_key[index]\r\n",
    "\r\n",
    "        # Get metatags and add them to meta name dict\r\n",
    "        metatags = soup.find_all('meta')\r\n",
    "        for tag in metatags:\r\n",
    "            try:\r\n",
    "                meta_name_dict[tag.attrs['name']] += 1\r\n",
    "            except:\r\n",
    "                continue\r\n",
    "        \r\n",
    "        # Create vector from meta name dict\r\n",
    "        name_vector = list(meta_name_dict.values())\r\n",
    "\r\n",
    "        dict_['index'].append(index)\r\n",
    "        dict_['label'].append(label)\r\n",
    "        dict_['site'].append(site)\r\n",
    "        dict_['name_vector'].append(name_vector)\r\n",
    "\r\n",
    "df = pd.DataFrame(dict_)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 2/2 [01:46<00:00, 53.42s/it]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "df_csv = pd.read_csv('html_data_expanded.csv')\r\n",
    "joined = df_csv.merge(df, on=['index', 'label'])\r\n",
    "joined.rename(columns={'Column1' : 'global_index', 'site_x' : 'site'}, inplace=True)\r\n",
    "joined.drop(columns=['site_y'], inplace=True)\r\n",
    "joined.to_csv('html_data.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## To get original meta tags\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from bs4 import BeautifulSoup\r\n",
    "import pandas as pd\r\n",
    "import os\r\n",
    "from pathlib import Path\r\n",
    "import numpy as np\r\n",
    "import pickle\r\n",
    "from tqdm import tqdm\r\n",
    "\r\n",
    "# Get url keys\r\n",
    "with open('real_url_key.pkl', 'rb') as f:\r\n",
    "    real_url_key = pickle.load(f)\r\n",
    "f.close()\r\n",
    "with open('fake_url_key.pkl', 'rb') as f:\r\n",
    "    fake_url_key = pickle.load(f)\r\n",
    "f.close()\r\n",
    "\r\n",
    "directory1 = Path.cwd() / 'HTML' / 'real'\r\n",
    "directory2 = Path.cwd() / 'HTML' / 'fake'\r\n",
    "\r\n",
    "directory = [directory1, directory2]\r\n",
    "dict_ = {'index' : [],\r\n",
    "        'label' : [],\r\n",
    "        'site' : [],\r\n",
    "        'og:title' : [],\r\n",
    "        'og:keywords' : [],\r\n",
    "        'og:description' : [],\r\n",
    "        'twitter:title' : [],\r\n",
    "        'twitter:keywords' : [],\r\n",
    "        'twitter:description' : [],\r\n",
    "        'keywords': [],\r\n",
    "        'description' : []}\r\n",
    "\r\n",
    "for dir_ in tqdm(directory):\r\n",
    "    for html in os.listdir(dir_):\r\n",
    "        # Open html file as soup object\r\n",
    "        with open(str(dir_) + '/' + html, encoding='utf8') as f:\r\n",
    "            soup = BeautifulSoup(f, 'html.parser')\r\n",
    "        f.close()\r\n",
    "\r\n",
    "        # metadata\r\n",
    "        index = int(str(html).split('.')[0])\r\n",
    "        label = str(dir_)[-4:]\r\n",
    "        site = real_url_key[index] if str(dir_)[-4:] == 'real' else fake_url_key[index]\r\n",
    "        \r\n",
    "        # ---------------------------------------------------------------------------- #\r\n",
    "        #                             open graph meta tags                             #\r\n",
    "        # ---------------------------------------------------------------------------- #\r\n",
    "        \r\n",
    "        # og:title\r\n",
    "        title = soup.find(\"meta\", property=\"og:title\")\r\n",
    "        try:\r\n",
    "            og_title = title['content'] if title else np.nan\r\n",
    "        except KeyError as err:\r\n",
    "            og_title = np.nan\r\n",
    "            print('KeyError at index {} title'.format(index))\r\n",
    "            print('continuing...')\r\n",
    "        \r\n",
    "        # og:keywords\r\n",
    "        og_keywords = soup.find('meta', property='og:keywords')\r\n",
    "        try:\r\n",
    "            og_keywords = og_keywords['content'] if og_keywords else np.nan\r\n",
    "        except KeyError as err:\r\n",
    "            og_keywords = np.nan\r\n",
    "            print('KeyError at index {} og:keywords'.format(index))\r\n",
    "            print('continuing...')\r\n",
    "        \r\n",
    "        # og:description\r\n",
    "        og_description = soup.find('meta', property='og:description')\r\n",
    "        try:\r\n",
    "            og_description = og_description['content'] if og_description else np.nan\r\n",
    "        except KeyError as err:\r\n",
    "            og_description = np.nan\r\n",
    "            print('KeyError at index {} og:description'.format(index))\r\n",
    "            print('continuing...')\r\n",
    "\r\n",
    "\r\n",
    "        # ---------------------------------------------------------------------------- #\r\n",
    "        #                               twitter meta tags                              #\r\n",
    "        # ---------------------------------------------------------------------------- #\r\n",
    "        \r\n",
    "        # twitter:title\r\n",
    "        tw_title = soup.find(\"meta\", attrs={'name':\"twitter:title\"})\r\n",
    "        try:\r\n",
    "            tw_title = tw_title['content'] if tw_title else np.nan\r\n",
    "        except KeyError as err:\r\n",
    "            tw_title = np.nan\r\n",
    "            print('KeyError at index {} twitter:title'.format(index))\r\n",
    "            print('continuing...')\r\n",
    "        \r\n",
    "        # twitter:keywords\r\n",
    "        tw_keywords = soup.find('meta', attrs={'name':\"twitter:keywords\"})\r\n",
    "        try:\r\n",
    "            tw_keywords = tw_keywords['content'] if tw_keywords else np.nan\r\n",
    "        except KeyError as err:\r\n",
    "            tw_keywords = np.nan\r\n",
    "            print('KeyError at index {} twitter:keywords'.format(index))\r\n",
    "            print('continuing...')\r\n",
    "        \r\n",
    "        # twitter:description\r\n",
    "        tw_description = soup.find('meta', attrs={'name':\"twitter:description\"})\r\n",
    "        try:\r\n",
    "            tw_description = tw_description['content'] if tw_description else np.nan\r\n",
    "        except KeyError as err:\r\n",
    "            tw_description = np.nan\r\n",
    "            print('KeyError at index {} twitter:description'.format(index))\r\n",
    "            print('continuing...')\r\n",
    "\r\n",
    "        # ---------------------------------------------------------------------------- #\r\n",
    "        #                               regular meta tags                              #\r\n",
    "        # ---------------------------------------------------------------------------- #\r\n",
    "\r\n",
    "        # keywords, description\r\n",
    "        description = soup.find('meta', attrs={'name':\"description\"})\r\n",
    "        try:\r\n",
    "            description = description['content'] if description else np.nan\r\n",
    "        except KeyError as err:\r\n",
    "            description = np.nan\r\n",
    "            print('KeyError at index {} description'.format(index))\r\n",
    "            print('continuing...')\r\n",
    "        \r\n",
    "        keywords = soup.find('meta', attrs={'name':\"keywords\"})\r\n",
    "        try:\r\n",
    "            keywords = keywords['content'] if keywords else np.nan\r\n",
    "        except KeyError as err:\r\n",
    "            keywords = np.nan\r\n",
    "            print('KeyError at index {} keywords'.format(index))\r\n",
    "            print('continuing...')\r\n",
    "\r\n",
    "        dict_['index'].append(index)\r\n",
    "        dict_['label'].append(label)\r\n",
    "        dict_['site'].append(site)\r\n",
    "        dict_['og:title'].append(og_title)\r\n",
    "        dict_['og:description'].append(og_description)\r\n",
    "        dict_['og:keywords'].append(og_keywords)\r\n",
    "        dict_['twitter:description'].append(tw_description)\r\n",
    "        dict_['twitter:title'].append(tw_title)\r\n",
    "        dict_['twitter:keywords'].append(tw_keywords)\r\n",
    "        dict_['keywords'].append(keywords)\r\n",
    "        dict_['description'].append(description)\r\n",
    "\r\n",
    "df = pd.DataFrame(dict_)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "KeyError at index 1008 twitter:title\n",
      "continuing...\n",
      "KeyError at index 1008 twitter:description\n",
      "continuing...\n",
      "KeyError at index 102 description\n",
      "continuing...\n",
      "KeyError at index 263 keywords\n",
      "continuing...\n",
      "KeyError at index 482 keywords\n",
      "continuing...\n",
      "KeyError at index 489 twitter:title\n",
      "continuing...\n",
      "KeyError at index 489 twitter:description\n",
      "continuing...\n",
      "KeyError at index 498 keywords\n",
      "continuing...\n",
      "KeyError at index 500 keywords\n",
      "continuing...\n",
      "KeyError at index 57 description\n",
      "continuing...\n",
      "KeyError at index 579 og:description\n",
      "continuing...\n",
      "KeyError at index 579 twitter:description\n",
      "continuing...\n",
      "KeyError at index 581 keywords\n",
      "continuing...\n",
      "KeyError at index 6 keywords\n",
      "continuing...\n",
      "KeyError at index 633 og:description\n",
      "continuing...\n",
      "KeyError at index 700 description\n",
      "continuing...\n",
      "KeyError at index 80 description\n",
      "continuing...\n",
      "KeyError at index 82 description\n",
      "continuing...\n",
      "KeyError at index 909 title\n",
      "continuing...\n",
      "KeyError at index 909 twitter:title\n",
      "continuing...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 2/2 [04:15<00:00, 127.80s/it]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## old:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from bs4 import BeautifulSoup\r\n",
    "import pandas as pd\r\n",
    "import os\r\n",
    "from pathlib import Path\r\n",
    "import numpy as np\r\n",
    "import pickle\r\n",
    "from tqdm import tqdm\r\n",
    "\r\n",
    "# Get url keys\r\n",
    "with open('real_url_key.pkl', 'rb') as f:\r\n",
    "    real_url_key = pickle.load(f)\r\n",
    "f.close()\r\n",
    "with open('fake_url_key.pkl', 'rb') as f:\r\n",
    "    fake_url_key = pickle.load(f)\r\n",
    "f.close()\r\n",
    "\r\n",
    "directory1 = Path.cwd() / 'HTML' / 'real'\r\n",
    "directory2 = Path.cwd() / 'HTML' / 'fake'\r\n",
    "\r\n",
    "directory = [directory1, directory2]\r\n",
    "dict_ = {'index' : [],\r\n",
    "        'label' : [],\r\n",
    "        'site' : [],\r\n",
    "        'og:title' : [],\r\n",
    "        'keywords': [],\r\n",
    "        'description' : []}\r\n",
    "\r\n",
    "for dir_ in tqdm(directory):\r\n",
    "    for html in os.listdir(dir_):\r\n",
    "        # Open html file as soup object\r\n",
    "        with open(str(dir_) + '/' + html, encoding='utf8') as f:\r\n",
    "            soup = BeautifulSoup(f, 'html.parser')\r\n",
    "        f.close()\r\n",
    "\r\n",
    "        # metadata\r\n",
    "        index = int(str(html).split('.')[0])\r\n",
    "        label = str(dir_)[-4:]\r\n",
    "        site = real_url_key[index] if str(dir_)[-4:] == 'real' else fake_url_key[index]\r\n",
    "        \r\n",
    "        # og:title\r\n",
    "        title = soup.find(\"meta\", property=\"og:title\")\r\n",
    "        try:\r\n",
    "            og_title = title['content'] if title else np.nan\r\n",
    "        except KeyError as err:\r\n",
    "            og_title = np.nan\r\n",
    "            print('KeyError at index {} title'.format(index))\r\n",
    "            print('continuing...')\r\n",
    "\r\n",
    "        # keywords, description\r\n",
    "        meta = soup.find_all('meta')\r\n",
    "        keywords = np.nan\r\n",
    "        description = np.nan\r\n",
    "        for tag in meta:\r\n",
    "            if 'name' in tag.attrs.keys() and tag.attrs['name'].strip().lower() in ['description']:\r\n",
    "                try:\r\n",
    "                    description = tag.attrs['content']\r\n",
    "                except KeyError as err:\r\n",
    "                    print('KeyError at index {} description'.format(index))\r\n",
    "                    print('continuing...')\r\n",
    "                    pass\r\n",
    "            elif 'name' in tag.attrs.keys() and tag.attrs['name'].strip().lower() in ['keywords']:\r\n",
    "                try:\r\n",
    "                    keywords = tag.attrs['content']\r\n",
    "                except KeyError as err:\r\n",
    "                    print('KeyError at index {} keywords'.format(index))\r\n",
    "                    print('continuing...')\r\n",
    "                    pass\r\n",
    "\r\n",
    "        dict_['index'].append(index)\r\n",
    "        dict_['label'].append(label)\r\n",
    "        dict_['site'].append(site)\r\n",
    "        dict_['og:title'].append(og_title)\r\n",
    "        dict_['keywords'].append(keywords)\r\n",
    "        dict_['description'].append(description)\r\n",
    "\r\n",
    "df = pd.DataFrame(dict_)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "df.to_excel(\"html_data.xlsx\", sheet_name=\"sheet1\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "df.to_excel(\"html_data_expanded.xlsx\", sheet_name=\"sheet1\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1131efc7635b497546d7e8fbc76ad9d1f9d5d5d7857bcde935d6feea39d08984"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}