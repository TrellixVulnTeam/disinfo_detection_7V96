{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "with open(Path.cwd().parent / 'content_analysis' / 'sp_df_tokenized.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:32,  7.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError at index 1 keywords\n",
      "continuing...\n",
      "KeyError at index 2 keywords\n",
      "continuing...\n",
      "KeyError at index 4 keywords\n",
      "continuing...\n",
      "KeyError at index 6 keywords\n",
      "continuing...\n",
      "KeyError at index 7 keywords\n",
      "continuing...\n",
      "KeyError at index 8 keywords\n",
      "continuing...\n",
      "KeyError at index 9 keywords\n",
      "continuing...\n",
      "KeyError at index 11 keywords\n",
      "continuing...\n",
      "KeyError at index 13 keywords\n",
      "continuing...\n",
      "KeyError at index 14 keywords\n",
      "continuing...\n",
      "KeyError at index 15 keywords\n",
      "continuing...\n",
      "KeyError at index 17 keywords\n",
      "continuing...\n",
      "KeyError at index 18 keywords\n",
      "continuing...\n",
      "KeyError at index 19 keywords\n",
      "continuing...\n",
      "KeyError at index 20 keywords\n",
      "continuing...\n",
      "KeyError at index 21 keywords\n",
      "continuing...\n",
      "KeyError at index 24 keywords\n",
      "continuing...\n",
      "KeyError at index 25 keywords\n",
      "continuing...\n",
      "KeyError at index 26 keywords\n",
      "continuing...\n",
      "KeyError at index 27 keywords\n",
      "continuing...\n",
      "KeyError at index 30 keywords\n",
      "continuing...\n",
      "KeyError at index 32 keywords\n",
      "continuing...\n",
      "KeyError at index 34 keywords\n",
      "continuing...\n",
      "KeyError at index 36 keywords\n",
      "continuing...\n",
      "KeyError at index 37 keywords\n",
      "continuing...\n",
      "KeyError at index 39 keywords\n",
      "continuing...\n",
      "KeyError at index 41 keywords\n",
      "continuing...\n",
      "KeyError at index 42 keywords\n",
      "continuing...\n",
      "KeyError at index 43 keywords\n",
      "continuing...\n",
      "KeyError at index 44 keywords\n",
      "continuing...\n",
      "KeyError at index 45 keywords\n",
      "continuing...\n",
      "KeyError at index 48 keywords\n",
      "continuing...\n",
      "KeyError at index 51 keywords\n",
      "continuing...\n",
      "KeyError at index 54 keywords\n",
      "continuing...\n",
      "KeyError at index 56 keywords\n",
      "continuing...\n",
      "KeyError at index 57 keywords\n",
      "continuing...\n",
      "KeyError at index 58 keywords\n",
      "continuing...\n",
      "KeyError at index 60 keywords\n",
      "continuing...\n",
      "KeyError at index 63 keywords\n",
      "continuing...\n",
      "KeyError at index 65 keywords\n",
      "continuing...\n",
      "KeyError at index 66 keywords\n",
      "continuing...\n",
      "KeyError at index 68 keywords\n",
      "continuing...\n",
      "KeyError at index 69 keywords\n",
      "continuing...\n",
      "KeyError at index 70 keywords\n",
      "continuing...\n",
      "KeyError at index 71 keywords\n",
      "continuing...\n",
      "KeyError at index 74 keywords\n",
      "continuing...\n",
      "KeyError at index 75 keywords\n",
      "continuing...\n",
      "KeyError at index 77 keywords\n",
      "continuing...\n",
      "KeyError at index 79 keywords\n",
      "continuing...\n",
      "KeyError at index 81 keywords\n",
      "continuing...\n",
      "KeyError at index 85 keywords\n",
      "continuing...\n",
      "KeyError at index 87 keywords\n",
      "continuing...\n",
      "KeyError at index 88 keywords\n",
      "continuing...\n",
      "KeyError at index 89 keywords\n",
      "continuing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [01:16,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError at index 0 keywords\n",
      "continuing...\n",
      "KeyError at index 1 keywords\n",
      "continuing...\n",
      "KeyError at index 2 description\n",
      "continuing...\n",
      "KeyError at index 2 keywords\n",
      "continuing...\n",
      "KeyError at index 3 keywords\n",
      "continuing...\n",
      "KeyError at index 4 description\n",
      "continuing...\n",
      "KeyError at index 4 keywords\n",
      "continuing...\n",
      "KeyError at index 6 keywords\n",
      "continuing...\n",
      "KeyError at index 8 description\n",
      "continuing...\n",
      "KeyError at index 8 keywords\n",
      "continuing...\n",
      "KeyError at index 10 description\n",
      "continuing...\n",
      "KeyError at index 10 keywords\n",
      "continuing...\n",
      "KeyError at index 11 description\n",
      "continuing...\n",
      "KeyError at index 12 description\n",
      "continuing...\n",
      "KeyError at index 12 keywords\n",
      "continuing...\n",
      "KeyError at index 13 keywords\n",
      "continuing...\n",
      "KeyError at index 15 description\n",
      "continuing...\n",
      "KeyError at index 15 keywords\n",
      "continuing...\n",
      "KeyError at index 16 description\n",
      "continuing...\n",
      "KeyError at index 16 keywords\n",
      "continuing...\n",
      "KeyError at index 17 description\n",
      "continuing...\n",
      "KeyError at index 17 keywords\n",
      "continuing...\n",
      "KeyError at index 18 description\n",
      "continuing...\n",
      "KeyError at index 18 keywords\n",
      "continuing...\n",
      "KeyError at index 19 keywords\n",
      "continuing...\n",
      "KeyError at index 20 description\n",
      "continuing...\n",
      "KeyError at index 20 keywords\n",
      "continuing...\n",
      "KeyError at index 21 description\n",
      "continuing...\n",
      "KeyError at index 21 keywords\n",
      "continuing...\n",
      "KeyError at index 22 description\n",
      "continuing...\n",
      "KeyError at index 22 keywords\n",
      "continuing...\n",
      "KeyError at index 23 description\n",
      "continuing...\n",
      "KeyError at index 25 keywords\n",
      "continuing...\n",
      "KeyError at index 26 keywords\n",
      "continuing...\n",
      "KeyError at index 28 keywords\n",
      "continuing...\n",
      "KeyError at index 29 description\n",
      "continuing...\n",
      "KeyError at index 29 keywords\n",
      "continuing...\n",
      "KeyError at index 31 description\n",
      "continuing...\n",
      "KeyError at index 31 keywords\n",
      "continuing...\n",
      "KeyError at index 32 description\n",
      "continuing...\n",
      "KeyError at index 32 keywords\n",
      "continuing...\n",
      "KeyError at index 33 description\n",
      "continuing...\n",
      "KeyError at index 33 keywords\n",
      "continuing...\n",
      "KeyError at index 36 keywords\n",
      "continuing...\n",
      "KeyError at index 37 description\n",
      "continuing...\n",
      "KeyError at index 37 keywords\n",
      "continuing...\n",
      "KeyError at index 40 description\n",
      "continuing...\n",
      "KeyError at index 40 keywords\n",
      "continuing...\n",
      "KeyError at index 41 description\n",
      "continuing...\n",
      "KeyError at index 41 keywords\n",
      "continuing...\n",
      "KeyError at index 42 description\n",
      "continuing...\n",
      "KeyError at index 42 keywords\n",
      "continuing...\n",
      "KeyError at index 43 description\n",
      "continuing...\n",
      "KeyError at index 43 keywords\n",
      "continuing...\n",
      "KeyError at index 44 description\n",
      "continuing...\n",
      "KeyError at index 45 description\n",
      "continuing...\n",
      "KeyError at index 45 keywords\n",
      "continuing...\n",
      "KeyError at index 47 keywords\n",
      "continuing...\n",
      "KeyError at index 48 description\n",
      "continuing...\n",
      "KeyError at index 48 keywords\n",
      "continuing...\n",
      "KeyError at index 50 description\n",
      "continuing...\n",
      "KeyError at index 50 keywords\n",
      "continuing...\n",
      "KeyError at index 51 description\n",
      "continuing...\n",
      "KeyError at index 51 keywords\n",
      "continuing...\n",
      "KeyError at index 52 description\n",
      "continuing...\n",
      "KeyError at index 52 keywords\n",
      "continuing...\n",
      "KeyError at index 53 keywords"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [01:21,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "continuing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [03:03, 11.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError at index 33 og:description\n",
      "continuing...\n",
      "KeyError at index 33 twitter:description\n",
      "continuing...\n",
      "KeyError at index 60 og:description\n",
      "continuing...\n",
      "KeyError at index 60 twitter:description\n",
      "continuing...\n",
      "KeyError at index 73 og:description\n",
      "continuing...\n",
      "KeyError at index 73 twitter:description\n",
      "continuing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [03:19,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError at index 0 keywords\n",
      "continuing...\n",
      "KeyError at index 1 keywords\n",
      "continuing...\n",
      "KeyError at index 3 description\n",
      "continuing...\n",
      "KeyError at index 3 keywords\n",
      "continuing...\n",
      "KeyError at index 4 keywords\n",
      "continuing...\n",
      "KeyError at index 5 description\n",
      "continuing...\n",
      "KeyError at index 5 keywords\n",
      "continuing...\n",
      "KeyError at index 8 keywords\n",
      "continuing...\n",
      "KeyError at index 9 description\n",
      "continuing...\n",
      "KeyError at index 9 keywords\n",
      "continuing...\n",
      "KeyError at index 12 description\n",
      "continuing...\n",
      "KeyError at index 12 keywords\n",
      "continuing...\n",
      "KeyError at index 13 description\n",
      "continuing...\n",
      "KeyError at index 13 keywords\n",
      "continuing...\n",
      "KeyError at index 14 description\n",
      "continuing...\n",
      "KeyError at index 15 keywords\n",
      "continuing...\n",
      "KeyError at index 16 description\n",
      "continuing...\n",
      "KeyError at index 16 keywords\n",
      "continuing...\n",
      "KeyError at index 17 description\n",
      "continuing...\n",
      "KeyError at index 17 keywords\n",
      "continuing...\n",
      "KeyError at index 18 description\n",
      "continuing...\n",
      "KeyError at index 18 keywords\n",
      "continuing...\n",
      "KeyError at index 19 description\n",
      "continuing...\n",
      "KeyError at index 19 keywords\n",
      "continuing...\n",
      "KeyError at index 20 keywords\n",
      "continuing...\n",
      "KeyError at index 21 description\n",
      "continuing...\n",
      "KeyError at index 21 keywords\n",
      "continuing...\n",
      "KeyError at index 22 description\n",
      "continuing...\n",
      "KeyError at index 22 keywords\n",
      "continuing...\n",
      "KeyError at index 23 description\n",
      "continuing...\n",
      "KeyError at index 24 description\n",
      "continuing...\n",
      "KeyError at index 24 keywords\n",
      "continuing...\n",
      "KeyError at index 26 keywords\n",
      "continuing...\n",
      "KeyError at index 27 keywords\n",
      "continuing...\n",
      "KeyError at index 28 keywords\n",
      "continuing...\n",
      "KeyError at index 29 description\n",
      "continuing...\n",
      "KeyError at index 29 keywords\n",
      "continuing...\n",
      "KeyError at index 30 description\n",
      "continuing...\n",
      "KeyError at index 30 keywords\n",
      "continuing...\n",
      "KeyError at index 31 description\n",
      "continuing...\n",
      "KeyError at index 31 keywords\n",
      "continuing...\n",
      "KeyError at index 32 description\n",
      "continuing...\n",
      "KeyError at index 32 keywords\n",
      "continuing...\n",
      "KeyError at index 35 description\n",
      "continuing...\n",
      "KeyError at index 35 keywords\n",
      "continuing...\n",
      "KeyError at index 36 keywords\n",
      "continuing...\n",
      "KeyError at index 37 description\n",
      "continuing...\n",
      "KeyError at index 37 keywords\n",
      "continuing...\n",
      "KeyError at index 38 description\n",
      "continuing...\n",
      "KeyError at index 38 keywords\n",
      "continuing...\n",
      "KeyError at index 42 description\n",
      "continuing...\n",
      "KeyError at index 42 keywords\n",
      "continuing...\n",
      "KeyError at index 43 description\n",
      "continuing...\n",
      "KeyError at index 43 keywords\n",
      "continuing...\n",
      "KeyError at index 45 description\n",
      "continuing...\n",
      "KeyError at index 45 keywords\n",
      "continuing...\n",
      "KeyError at index 46 description\n",
      "continuing...\n",
      "KeyError at index 46 keywords\n",
      "continuing...\n",
      "KeyError at index 47 description\n",
      "continuing...\n",
      "KeyError at index 47 keywords\n",
      "continuing...\n",
      "KeyError at index 48 description\n",
      "continuing...\n",
      "KeyError at index 49 description\n",
      "continuing...\n",
      "KeyError at index 49 keywords\n",
      "continuing...\n",
      "KeyError at index 51 keywords\n",
      "continuing...\n",
      "KeyError at index 52 description\n",
      "continuing...\n",
      "KeyError at index 52 keywords\n",
      "continuing...\n",
      "KeyError at index 54 description\n",
      "continuing...\n",
      "KeyError at index 54 keywords\n",
      "continuing...\n",
      "KeyError at index 55 description\n",
      "continuing...\n",
      "KeyError at index 55 keywords\n",
      "continuing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29it [03:25,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError at index 56 description\n",
      "continuing...\n",
      "KeyError at index 56 keywords\n",
      "continuing...\n",
      "KeyError at index 57 keywords\n",
      "continuing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [07:01,  6.91s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "def create_dir(target_directory):\n",
    "    '''Creates directory target_directory if the directory doesn't already exist'''\n",
    "    if not os.path.isdir(str(target_directory)):\n",
    "        os.mkdir(target_directory)\n",
    "        \n",
    "def populate_meta_tags(source_dir, target_dir):\n",
    "    '''\n",
    "    Populates target dir with .txt files of scraped meta tags from source dir\n",
    "    Each .txt file contains all visible text\n",
    "\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ARGUMENTS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #\n",
    "    - source_dir\n",
    "        - source of HTML files, should be raw_sources/<real or fake>/html\n",
    "    - target_dir\n",
    "        - directory to save parsed files\n",
    "    '''\n",
    "    for idx, dir_ in tqdm(enumerate([source_dir / s for s in os.listdir(source_dir)])):\n",
    "        files = [dir_ / s for s in os.listdir(dir_)]\n",
    "        dest_dir = target_dir / dir_.stem\n",
    "        EXISTS = create_dir(dest_dir)\n",
    "        if EXISTS: # Skip directoriers that already exist\n",
    "            continue\n",
    "        for index, html in enumerate(files):\n",
    "            try:\n",
    "                # Open HTML\n",
    "                with open(html, encoding='utf8') as f:\n",
    "                    soup = BeautifulSoup(f, 'html.parser')\n",
    "                f.close()\n",
    "                \n",
    "                # ---------------------------------------------------------------------------- #\n",
    "                #                             open graph meta tags                             #\n",
    "                # ---------------------------------------------------------------------------- #\n",
    "                \n",
    "                # og:title\n",
    "                title = soup.find(\"meta\", property=\"og:title\")\n",
    "                try:\n",
    "                    og_title = title['content'] if title else ''\n",
    "                except KeyError:\n",
    "                    og_title = ''\n",
    "                    print('KeyError at index {} title'.format(index))\n",
    "                    print('continuing...')\n",
    "                \n",
    "                # og:keywords\n",
    "                og_keywords = soup.find('meta', property='og:keywords')\n",
    "                try:\n",
    "                    og_keywords = og_keywords['content'] if og_keywords else ''\n",
    "                except KeyError:\n",
    "                    og_keywords = ''\n",
    "                    print('KeyError at index {} og:keywords'.format(index))\n",
    "                    print('continuing...')\n",
    "                \n",
    "                # og:description\n",
    "                og_description = soup.find('meta', property='og:description')\n",
    "                try:\n",
    "                    og_description = og_description['content'] if og_description else ''\n",
    "                except KeyError:\n",
    "                    og_description = ''\n",
    "                    print('KeyError at index {} og:description'.format(index))\n",
    "                    print('continuing...')\n",
    "\n",
    "\n",
    "                # ---------------------------------------------------------------------------- #\n",
    "                #                               twitter meta tags                              #\n",
    "                # ---------------------------------------------------------------------------- #\n",
    "                \n",
    "                # twitter:title\n",
    "                tw_title = soup.find(\"meta\", attrs={'name':\"twitter:title\"})\n",
    "                try:\n",
    "                    tw_title = tw_title['content'] if tw_title else ''\n",
    "                except KeyError:\n",
    "                    tw_title = ''\n",
    "                    print('KeyError at index {} twitter:title'.format(index))\n",
    "                    print('continuing...')\n",
    "                \n",
    "                # twitter:keywords\n",
    "                tw_keywords = soup.find('meta', attrs={'name':\"twitter:keywords\"})\n",
    "                try:\n",
    "                    tw_keywords = tw_keywords['content'] if tw_keywords else ''\n",
    "                except KeyError:\n",
    "                    tw_keywords = ''\n",
    "                    print('KeyError at index {} twitter:keywords'.format(index))\n",
    "                    print('continuing...')\n",
    "                \n",
    "                # twitter:description\n",
    "                tw_description = soup.find('meta', attrs={'name':\"twitter:description\"})\n",
    "                try:\n",
    "                    tw_description = tw_description['content'] if tw_description else ''\n",
    "                except KeyError:\n",
    "                    tw_description = ''\n",
    "                    print('KeyError at index {} twitter:description'.format(index))\n",
    "                    print('continuing...')\n",
    "\n",
    "                # ---------------------------------------------------------------------------- #\n",
    "                #                               regular meta tags                              #\n",
    "                # ---------------------------------------------------------------------------- #\n",
    "\n",
    "                # keywords, description\n",
    "                description = soup.find('meta', attrs={'name':\"description\"})\n",
    "                try:\n",
    "                    description = description['content'] if description else ''\n",
    "                except KeyError:\n",
    "                    description = ''\n",
    "                    print('KeyError at index {} description'.format(index))\n",
    "                    print('continuing...')\n",
    "                \n",
    "                keywords = soup.find('meta', attrs={'name':\"keywords\"})\n",
    "                try:\n",
    "                    keywords = keywords['content'] if keywords else ''\n",
    "                except KeyError:\n",
    "                    keywords = ''\n",
    "                    print('KeyError at index {} keywords'.format(index))\n",
    "                    print('continuing...')\n",
    "\n",
    "\n",
    "                # print(f'\\nkeywords {keywords}')\n",
    "                # print(f'description {description}')\n",
    "                # print(f'og_title {og_title}')\n",
    "                # print(f'og_description {og_description}')\n",
    "                # print(f'og_keywords {og_keywords}')\n",
    "                # print(f'tw_description {tw_description}')\n",
    "                # print(f'tw_title {tw_title}')\n",
    "                # print(f'tw_keywords {tw_keywords}')\n",
    "\n",
    "                file_name = html.stem.split('.')[0] + '.txt'\n",
    "                file_path = dest_dir / file_name\n",
    "                text = [keywords, description, og_title, og_description, og_keywords, tw_description, tw_title, tw_keywords]\n",
    "                if any(x != '' for x in text):\n",
    "                    with open(file_path, 'a', encoding='utf8') as f:\n",
    "                        f.writelines(text)\n",
    "                    f.close()\n",
    "            except:\n",
    "                print(f'ERROR processing {dest_dir}, continuing...')\n",
    "                continue\n",
    "    return\n",
    "\n",
    "real_source_dir = Path.cwd().parent / 'sp_sources'\n",
    "real_target_dir = Path.cwd() / 'meta_tags_parsed'\n",
    "\n",
    "populate_meta_tags(real_source_dir, real_target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no content at c:\\Users\\ewais\\Documents\\GitHub\\misinfo_detection\\sydney_powell_analysis\\meta tag analysis\\meta_tags_parsed\\77\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>site</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>meta_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>dailyexpose.uk</td>\n",
       "      <td>[…]      Does the Covid-19 Virus contain Genet...</td>\n",
       "      <td>[…, doe, covid, viru, contain, genet, sequenc,...</td>\n",
       "      <td>Read all of the posts by Rhoda Wilson on The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>rumble.com</td>\n",
       "      <td>Note\\n \\t\\t\\tthat this Policy may be modified ...</td>\n",
       "      <td>[note, thi, polici, may, modifi, time, time, s...</td>\n",
       "      <td>The 2022 NBA Playoffs are here as the NBA Pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>harpers.org</td>\n",
       "      <td>It wasn’t just about the PowerPoint, though; i...</td>\n",
       "      <td>[’, powerpoint, though, retrospect, powerpoint...</td>\n",
       "      <td>Harper's Magazine, the oldest general-interes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>kanekoa.substack.com</td>\n",
       "      <td>The pathologist cited “rare, severe side effec...</td>\n",
       "      <td>[pathologist, cite, “, rare, sever, side, effe...</td>\n",
       "      <td>True the Vote used two petabytes of data, ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>pattyporter.net</td>\n",
       "      <td>Support local candidates running for office wi...</td>\n",
       "      <td>[support, local, candid, run, offic, boot, gro...</td>\n",
       "      <td>Border CrisisBorder Crisis - HomeBorder Crisi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                  site  \\\n",
       "0      0        dailyexpose.uk   \n",
       "1      1            rumble.com   \n",
       "2      2           harpers.org   \n",
       "3      3  kanekoa.substack.com   \n",
       "4      6       pattyporter.net   \n",
       "\n",
       "                                                text  \\\n",
       "0  […]      Does the Covid-19 Virus contain Genet...   \n",
       "1  Note\\n \\t\\t\\tthat this Policy may be modified ...   \n",
       "2  It wasn’t just about the PowerPoint, though; i...   \n",
       "3  The pathologist cited “rare, severe side effec...   \n",
       "4  Support local candidates running for office wi...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  […, doe, covid, viru, contain, genet, sequenc,...   \n",
       "1  [note, thi, polici, may, modifi, time, time, s...   \n",
       "2  [’, powerpoint, though, retrospect, powerpoint...   \n",
       "3  [pathologist, cite, “, rare, sever, side, effe...   \n",
       "4  [support, local, candid, run, offic, boot, gro...   \n",
       "\n",
       "                                           meta_text  \n",
       "0   Read all of the posts by Rhoda Wilson on The ...  \n",
       "1   The 2022 NBA Playoffs are here as the NBA Pla...  \n",
       "2   Harper's Magazine, the oldest general-interes...  \n",
       "3   True the Vote used two petabytes of data, ten...  \n",
       "4   Border CrisisBorder Crisis - HomeBorder Crisi...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_list = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    dir_ = Path.cwd() / 'meta_tags_parsed' / str(row['index'])\n",
    "    \n",
    "    content = ''\n",
    "    for text in [dir_ / x for x in os.listdir(dir_)]:\n",
    "        with open(text, 'r', encoding='utf8') as f:\n",
    "            lines = f.readlines()\n",
    "        f.close()\n",
    "\n",
    "        page_content = ' '.join(lines)\n",
    "\n",
    "        content = content + ' ' + page_content\n",
    "    if content == '':\n",
    "        print(f'no content at {dir_}')\n",
    "        content += 'none'\n",
    "    content_list.append(content)\n",
    "\n",
    "df['meta_text'] = content_list\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sp_df_tokenized.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1131efc7635b497546d7e8fbc76ad9d1f9d5d5d7857bcde935d6feea39d08984"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
