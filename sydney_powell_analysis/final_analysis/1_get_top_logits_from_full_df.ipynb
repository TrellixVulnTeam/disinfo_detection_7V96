{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takes full_df (containing all features of the scraped data), creates classifiers and uses them to find the top logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>site</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>dailyexpose.uk</td>\n",
       "      <td>[…]      Does the Covid-19 Virus contain Genet...</td>\n",
       "      <td>[…, doe, covid, viru, contain, genet, sequenc,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>rumble.com</td>\n",
       "      <td>Note\\n \\t\\t\\tthat this Policy may be modified ...</td>\n",
       "      <td>[note, thi, polici, may, modifi, time, time, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>harpers.org</td>\n",
       "      <td>It wasn’t just about the PowerPoint, though; i...</td>\n",
       "      <td>[’, powerpoint, though, retrospect, powerpoint...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>kanekoa.substack.com</td>\n",
       "      <td>The pathologist cited “rare, severe side effec...</td>\n",
       "      <td>[pathologist, cite, “, rare, sever, side, effe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>pattyporter.net</td>\n",
       "      <td>Support local candidates running for office wi...</td>\n",
       "      <td>[support, local, candid, run, offic, boot, gro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                  site  \\\n",
       "0      0        dailyexpose.uk   \n",
       "1      1            rumble.com   \n",
       "2      2           harpers.org   \n",
       "3      3  kanekoa.substack.com   \n",
       "4      6       pattyporter.net   \n",
       "\n",
       "                                                text  \\\n",
       "0  […]      Does the Covid-19 Virus contain Genet...   \n",
       "1  Note\\n \\t\\t\\tthat this Policy may be modified ...   \n",
       "2  It wasn’t just about the PowerPoint, though; i...   \n",
       "3  The pathologist cited “rare, severe side effec...   \n",
       "4  Support local candidates running for office wi...   \n",
       "\n",
       "                                           tokenized  \n",
       "0  […, doe, covid, viru, contain, genet, sequenc,...  \n",
       "1  [note, thi, polici, may, modifi, time, time, s...  \n",
       "2  [’, powerpoint, though, retrospect, powerpoint...  \n",
       "3  [pathologist, cite, “, rare, sever, side, effe...  \n",
       "4  [support, local, candid, run, offic, boot, gro...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# First, load the dataframe\n",
    "with open(Path.cwd() / 'sp_df_tokenized.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('content_logit_tokens.pkl', 'rb') as f:\n",
    "    content_logit_tokens = pickle.load(f)\n",
    "with open('meta_logit_tokens.pkl', 'rb') as f:\n",
    "    meta_logit_tokens = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87650\n",
      "14663\n"
     ]
    }
   ],
   "source": [
    "print(len(content_logit_tokens))\n",
    "print(len(meta_logit_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "def vocab_list_from_logit_tokens(logit_tokens, num_tokens=100):\n",
    "    '''\n",
    "    Takes logit_tokens output by logit_explained_variance() and creates vocab_list for vocabify_dataframe\n",
    "    ignores non-alphanumeric tokens\n",
    "    '''\n",
    "    vocab_list = []\n",
    "    i = 0\n",
    "    while len(vocab_list) < num_tokens:\n",
    "        i += 1\n",
    "        non_ascii = False\n",
    "        for x in logit_tokens[i]:\n",
    "            for char in x:\n",
    "                if ord(char) < 65 or ord(char) > 122:\n",
    "                    non_ascii = True\n",
    "        if not non_ascii:\n",
    "            vocab_list.append(logit_tokens[i])\n",
    "    return vocab_list\n",
    "\n",
    "content_vocab_list_500 = vocab_list_from_logit_tokens(content_logit_tokens, num_tokens=500)\n",
    "meta_vocab_list_500 = vocab_list_from_logit_tokens(meta_logit_tokens, num_tokens=500)\n",
    "\n",
    "print(content_vocab_list_500)\n",
    "print(meta_vocab_list_500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing tokenized into reduced_content_500\n",
      "\n",
      "tokenizing meta_tokenized into reduced_meta_500\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'meta_tokenized'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2894\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2895\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'meta_tokenized'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-a3af729289af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabify_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tokenized'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'reduced_content_500'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontent_vocab_list_500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabify_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'meta_tokenized'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'reduced_meta_500'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeta_vocab_list_500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-a3af729289af>\u001b[0m in \u001b[0;36mvocabify_dataframe\u001b[1;34m(df, col, new_col_name, vocab_list)\u001b[0m\n\u001b[0;32m     10\u001b[0m     '''\n\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'\\ntokenizing {col} into {new_col_name}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_col_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvocab_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[0;32m   7546\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7547\u001b[0m         )\n\u001b[1;32m-> 7548\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7550\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DataFrame\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 271\u001b[1;33m         \u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m         \u001b[1;31m# wrap results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m                     \u001b[1;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                         \u001b[1;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-a3af729289af>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m     10\u001b[0m     '''\n\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'\\ntokenizing {col} into {new_col_name}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_col_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvocab_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    880\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 882\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[1;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 989\u001b[1;33m         \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    990\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2895\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2899\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'meta_tokenized'"
     ]
    }
   ],
   "source": [
    "from analysis_functions import *\n",
    "\n",
    "def vocabify_dataframe(df, col, new_col_name, vocab_list):\n",
    "    '''\n",
    "    vocab_list : list of tokens to filter df[col] with\n",
    "\n",
    "    takes a dataframe, and creates a new column ('reduced_tokens') from 'tokenized'. Basically filters the list of tokens in 'tokenized' to only include that in 'vocab_list'\n",
    "\n",
    "    after vocabifying dataframe, should be able to train logistic regression with reduced dimensionality vector\n",
    "    '''\n",
    "    print(f'\\ntokenizing {col} into {new_col_name}')\n",
    "    df[new_col_name] = df.apply(lambda row: [x for x in row[col] if x in vocab_list], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = vocabify_dataframe(df, 'tokenized', 'reduced_content_500', content_vocab_list_500)\n",
    "df = vocabify_dataframe(df, 'meta_tokenized', 'reduced_meta_500', meta_vocab_list_500)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get TF-IDF representations of the proper columns\n",
    "\n",
    "meta_tfidf_100, meta_vocab_100, _ = tfidf_transformation(df, 'reduced_meta_100')\n",
    "content_tfidf_100, content_vocab_100, _ = tfidf_transformation(df, 'reduced_content_100')\n",
    "meta_tfidf_500, meta_vocab_500, _ = tfidf_transformation(df, 'reduced_meta_500')\n",
    "content_tfidf_500, content_vocab_500, _ = tfidf_transformation(df, 'reduced_content_500')\n",
    "meta_tfidf_1000, meta_vocab_1000, _ = tfidf_transformation(df, 'reduced_meta_1000')\n",
    "content_tfidf_1000, content_vocab_1000, _ = tfidf_transformation(df, 'reduced_content_1000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site</th>\n",
       "      <th>y</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>meta_tokenized</th>\n",
       "      <th>global_index</th>\n",
       "      <th>subindex</th>\n",
       "      <th>domain</th>\n",
       "      <th>label</th>\n",
       "      <th>vectorized_links</th>\n",
       "      <th>reduced_content</th>\n",
       "      <th>...</th>\n",
       "      <th>reduced_content_500</th>\n",
       "      <th>reduced_meta_500</th>\n",
       "      <th>reduced_content_1000</th>\n",
       "      <th>reduced_meta_1000</th>\n",
       "      <th>meta_tfidf_100</th>\n",
       "      <th>content_tfidf_100</th>\n",
       "      <th>meta_tfidf_500</th>\n",
       "      <th>content_tfidf_500</th>\n",
       "      <th>meta_tfidf_1000</th>\n",
       "      <th>content_tfidf_1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.google.com</td>\n",
       "      <td>0</td>\n",
       "      <td>[robust, integr, connect, cowork, via, googl, ...</td>\n",
       "      <td>[offici, gmail, help, center, find, tip, tutor...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>.google.com</td>\n",
       "      <td>real</td>\n",
       "      <td>[0.18734177215189873, 0.0, 10.0]</td>\n",
       "      <td>[googl, googl, cooki, product, googl, account,...</td>\n",
       "      <td>...</td>\n",
       "      <td>[googl, googl, action, list, shop, cooki, mani...</td>\n",
       "      <td>[offici, help, find, use, answer, question, br...</td>\n",
       "      <td>[googl, googl, chat, send, invit, action, list...</td>\n",
       "      <td>[offici, help, find, use, answer, frequent, qu...</td>\n",
       "      <td>[0.00013044355184170067, 0.0, 0.00020095475506...</td>\n",
       "      <td>[0.0009109320298328371, 0.0029521434893754624,...</td>\n",
       "      <td>[0.00013044355184170067, 0.0, 0.00036633413545...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0004451551200154094, 0.00144...</td>\n",
       "      <td>[0.0, 0.00013044355184170067, 0.0, 0.0, 0.0003...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0126034675947897e-05, 0.0004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.youtube.com</td>\n",
       "      <td>0</td>\n",
       "      <td>[doctor, mike, doctor, mike, doctor, mike, ver...</td>\n",
       "      <td>[video, live, stream, live, video, cbsn, covid...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>.youtube.com</td>\n",
       "      <td>real</td>\n",
       "      <td>[0.2180906622101525, 0.0, 10.0]</td>\n",
       "      <td>[current, googl, scroll, scroll, access, googl...</td>\n",
       "      <td>...</td>\n",
       "      <td>[mike, mike, mike, play, live, play, live, rec...</td>\n",
       "      <td>[live, live, coronaviru, vaccin, access, vacci...</td>\n",
       "      <td>[mike, mike, mike, verifi, view, play, verifi,...</td>\n",
       "      <td>[live, live, coronaviru, vaccin, access, vacci...</td>\n",
       "      <td>[0.00026088710368340134, 0.0, 0.00015071606629...</td>\n",
       "      <td>[0.0007327061979090211, 0.0010234097429834936,...</td>\n",
       "      <td>[0.00026088710368340134, 0.0, 0.0, 0.000150716...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.00035805955305587284, 0.0005...</td>\n",
       "      <td>[0.0, 0.00026088710368340134, 0.0, 0.0, 0.0, 0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 5.063017337973948e-05, 0.00035...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.baidu.com</td>\n",
       "      <td>0</td>\n",
       "      <td>[åström, karl, johan, iee, review, micropor, m...</td>\n",
       "      <td>[investor, relat, websit, contain, inform, bai...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>.baidu.com</td>\n",
       "      <td>real</td>\n",
       "      <td>[0.024390243902439025, 0.0, 10.0]</td>\n",
       "      <td>[product]</td>\n",
       "      <td>...</td>\n",
       "      <td>[review, institut, health, health, transact, e...</td>\n",
       "      <td>[investor, websit, inform, busi, stockhold, in...</td>\n",
       "      <td>[review, institut, health, health, food, scien...</td>\n",
       "      <td>[investor, websit, inform, busi, stockhold, in...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.6462038087577...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.qq.com</td>\n",
       "      <td>0</td>\n",
       "      <td>[scan, bind, meanwhil, must, inform, exist, me...</td>\n",
       "      <td>[none]</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>.qq.com</td>\n",
       "      <td>real</td>\n",
       "      <td>[0.0, 0.0, 3.0]</td>\n",
       "      <td>[code, code, code, account, code, account, error]</td>\n",
       "      <td>...</td>\n",
       "      <td>[agreement, join, registr, also, registr, agre...</td>\n",
       "      <td>[none]</td>\n",
       "      <td>[agreement, invit, join, registr, red, also, r...</td>\n",
       "      <td>[none]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 3.936191319167283e-05, 0.0, 1.9615273021...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 1.9235416712804245e-05, 0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0126034675947897e-05, 0.0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.facebook.com</td>\n",
       "      <td>0</td>\n",
       "      <td>[discrimin, base, upon, race, religion, color,...</td>\n",
       "      <td>[facebook, compani, meta, build, technolog, he...</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>.facebook.com</td>\n",
       "      <td>real</td>\n",
       "      <td>[0.2856230031948882, 0.0, 10.0]</td>\n",
       "      <td>[ad, data, english, upload, ad, product, data,...</td>\n",
       "      <td>...</td>\n",
       "      <td>[color, includ, health, relat, medic, protect,...</td>\n",
       "      <td>[facebook, compani, build, help, connect, find...</td>\n",
       "      <td>[color, origin, includ, health, relat, medic, ...</td>\n",
       "      <td>[facebook, compani, build, technolog, help, co...</td>\n",
       "      <td>[0.0, 0.0, 5.023868876613474e-05, 0.0, 0.0, 0....</td>\n",
       "      <td>[0.0005148746255576905, 0.0014170288749002218,...</td>\n",
       "      <td>[0.0, 0.0, 9.158353386307677e-05, 5.0238688766...</td>\n",
       "      <td>[0.0, 0.0, 3.4364342205938227e-05, 0.000251609...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 9.158353386307677e-05, 0....</td>\n",
       "      <td>[0.0, 0.0, 3.4364342205938227e-05, 1.012603467...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      site  y  \\\n",
       "0    http://www.google.com  0   \n",
       "1   http://www.youtube.com  0   \n",
       "2     http://www.baidu.com  0   \n",
       "3        http://www.qq.com  0   \n",
       "4  http://www.facebook.com  0   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [robust, integr, connect, cowork, via, googl, ...   \n",
       "1  [doctor, mike, doctor, mike, doctor, mike, ver...   \n",
       "2  [åström, karl, johan, iee, review, micropor, m...   \n",
       "3  [scan, bind, meanwhil, must, inform, exist, me...   \n",
       "4  [discrimin, base, upon, race, religion, color,...   \n",
       "\n",
       "                                      meta_tokenized global_index subindex  \\\n",
       "0  [offici, gmail, help, center, find, tip, tutor...            0        0   \n",
       "1  [video, live, stream, live, video, cbsn, covid...            1        1   \n",
       "2  [investor, relat, websit, contain, inform, bai...            2        3   \n",
       "3                                             [none]            3        4   \n",
       "4  [facebook, compani, meta, build, technolog, he...            4        7   \n",
       "\n",
       "          domain label                   vectorized_links  \\\n",
       "0    .google.com  real   [0.18734177215189873, 0.0, 10.0]   \n",
       "1   .youtube.com  real    [0.2180906622101525, 0.0, 10.0]   \n",
       "2     .baidu.com  real  [0.024390243902439025, 0.0, 10.0]   \n",
       "3        .qq.com  real                    [0.0, 0.0, 3.0]   \n",
       "4  .facebook.com  real    [0.2856230031948882, 0.0, 10.0]   \n",
       "\n",
       "                                     reduced_content  ...  \\\n",
       "0  [googl, googl, cooki, product, googl, account,...  ...   \n",
       "1  [current, googl, scroll, scroll, access, googl...  ...   \n",
       "2                                          [product]  ...   \n",
       "3  [code, code, code, account, code, account, error]  ...   \n",
       "4  [ad, data, english, upload, ad, product, data,...  ...   \n",
       "\n",
       "                                 reduced_content_500  \\\n",
       "0  [googl, googl, action, list, shop, cooki, mani...   \n",
       "1  [mike, mike, mike, play, live, play, live, rec...   \n",
       "2  [review, institut, health, health, transact, e...   \n",
       "3  [agreement, join, registr, also, registr, agre...   \n",
       "4  [color, includ, health, relat, medic, protect,...   \n",
       "\n",
       "                                    reduced_meta_500  \\\n",
       "0  [offici, help, find, use, answer, question, br...   \n",
       "1  [live, live, coronaviru, vaccin, access, vacci...   \n",
       "2  [investor, websit, inform, busi, stockhold, in...   \n",
       "3                                             [none]   \n",
       "4  [facebook, compani, build, help, connect, find...   \n",
       "\n",
       "                                reduced_content_1000  \\\n",
       "0  [googl, googl, chat, send, invit, action, list...   \n",
       "1  [mike, mike, mike, verifi, view, play, verifi,...   \n",
       "2  [review, institut, health, health, food, scien...   \n",
       "3  [agreement, invit, join, registr, red, also, r...   \n",
       "4  [color, origin, includ, health, relat, medic, ...   \n",
       "\n",
       "                                   reduced_meta_1000  \\\n",
       "0  [offici, help, find, use, answer, frequent, qu...   \n",
       "1  [live, live, coronaviru, vaccin, access, vacci...   \n",
       "2  [investor, websit, inform, busi, stockhold, in...   \n",
       "3                                             [none]   \n",
       "4  [facebook, compani, build, technolog, help, co...   \n",
       "\n",
       "                                      meta_tfidf_100  \\\n",
       "0  [0.00013044355184170067, 0.0, 0.00020095475506...   \n",
       "1  [0.00026088710368340134, 0.0, 0.00015071606629...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 5.023868876613474e-05, 0.0, 0.0, 0....   \n",
       "\n",
       "                                   content_tfidf_100  \\\n",
       "0  [0.0009109320298328371, 0.0029521434893754624,...   \n",
       "1  [0.0007327061979090211, 0.0010234097429834936,...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.0, 3.936191319167283e-05, 0.0, 1.9615273021...   \n",
       "4  [0.0005148746255576905, 0.0014170288749002218,...   \n",
       "\n",
       "                                      meta_tfidf_500  \\\n",
       "0  [0.00013044355184170067, 0.0, 0.00036633413545...   \n",
       "1  [0.00026088710368340134, 0.0, 0.0, 0.000150716...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 9.158353386307677e-05, 5.0238688766...   \n",
       "\n",
       "                                   content_tfidf_500  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0004451551200154094, 0.00144...   \n",
       "1  [0.0, 0.0, 0.0, 0.00035805955305587284, 0.0005...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.6462038087577...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 1.9235416712804245e-05, 0...   \n",
       "4  [0.0, 0.0, 3.4364342205938227e-05, 0.000251609...   \n",
       "\n",
       "                                     meta_tfidf_1000  \\\n",
       "0  [0.0, 0.00013044355184170067, 0.0, 0.0, 0.0003...   \n",
       "1  [0.0, 0.00026088710368340134, 0.0, 0.0, 0.0, 0...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 9.158353386307677e-05, 0....   \n",
       "\n",
       "                                  content_tfidf_1000  \n",
       "0  [0.0, 0.0, 0.0, 1.0126034675947897e-05, 0.0004...  \n",
       "1  [0.0, 0.0, 0.0, 5.063017337973948e-05, 0.00035...  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3  [0.0, 0.0, 0.0, 1.0126034675947897e-05, 0.0, 0...  \n",
       "4  [0.0, 0.0, 3.4364342205938227e-05, 1.012603467...  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_tfidf_vectors_to_dataframe(df, tfidf, new_col_name):\n",
    "    assert len(df) == tfidf.shape[0], 'ERROR: size mismatch'\n",
    "    \n",
    "    # Convert to array\n",
    "    tfidf = tfidf.toarray()\n",
    "    # Normalize\n",
    "    tfidf = tfidf / np.max(tfidf)\n",
    "\n",
    "    new_col = []\n",
    "    for i in range(tfidf.shape[0]):\n",
    "        new_col.append(tfidf[i,:])\n",
    "\n",
    "    df[new_col_name] = new_col\n",
    "    return df\n",
    "\n",
    "df = add_tfidf_vectors_to_dataframe(df, meta_tfidf_100, 'meta_tfidf_100')\n",
    "df = add_tfidf_vectors_to_dataframe(df, content_tfidf_100, 'content_tfidf_100')\n",
    "df = add_tfidf_vectors_to_dataframe(df, meta_tfidf_500, 'meta_tfidf_500')\n",
    "df = add_tfidf_vectors_to_dataframe(df, content_tfidf_500, 'content_tfidf_500')\n",
    "df = add_tfidf_vectors_to_dataframe(df, meta_tfidf_1000, 'meta_tfidf_1000')\n",
    "df = add_tfidf_vectors_to_dataframe(df, content_tfidf_1000, 'content_tfidf_1000')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate the optimal classifier performance for each of the reduced tfidif columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('full_df_with_reduced.pkl', 'wb') as f:\n",
    "#     pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "with open('full_df_with_reduced.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['site', 'y', 'tokenized', 'meta_tokenized', 'global_index', 'subindex',\n",
      "       'domain', 'label', 'vectorized_links', 'reduced_content',\n",
      "       'reduced_meta', 'meta_tfidf', 'content_tfidf', 'combined_features',\n",
      "       'reduced_content_100', 'reduced_meta_100', 'reduced_content_500',\n",
      "       'reduced_meta_500', 'reduced_content_1000', 'reduced_meta_1000',\n",
      "       'meta_tfidf_100', 'content_tfidf_100', 'meta_tfidf_500',\n",
      "       'content_tfidf_500', 'meta_tfidf_1000', 'content_tfidf_1000'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df['vec_links_normalized'] = df.apply(lambda row: (np.array(row.vectorized_links)+0.5)/10.5, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "best estimator:\n",
      "Pipeline(steps=[('classifier',\n",
      "                 LogisticRegression(C=29.763514416313132, penalty='l1',\n",
      "                                    solver='liblinear'))])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "def grid_search_logistic_regression(df, col, param_grid = None, col_is_tfidf=False):\n",
    "    if not param_grid:\n",
    "        param_grid = [\n",
    "                     {'classifier' : [LogisticRegression()],\n",
    "                      'classifier__penalty' : ['l1', 'l2'],\n",
    "                      # 'classifier__C' : np.logspace(-1, 4, 20),\n",
    "                      'classifier__C' : np.logspace(-4, 4, 20),\n",
    "                      'classifier__solver' : ['liblinear']}\n",
    "                      ]\n",
    "\n",
    "    if not col_is_tfidf:\n",
    "        X_tfidf, tf_vocab, X = tfidf_transformation(df, col)\n",
    "    else:\n",
    "        X_tfidf = np.array(df[col].tolist())\n",
    "    y = df['y'].to_numpy()\n",
    "    X_train, _, y_train, _ = train_test_split(X_tfidf, y)\n",
    "\n",
    "    pipe = Pipeline([('classifier' , LogisticRegression(max_iter=1000))])\n",
    "\n",
    "    model = GridSearchCV(pipe, param_grid=param_grid, cv=5, verbose=3, n_jobs=-1).fit(X_train, y_train)\n",
    "    best_model = model.fit(X_train, y_train)\n",
    "\n",
    "    print('best estimator:')\n",
    "    print(best_model.best_estimator_)\n",
    "    return\n",
    "\n",
    "grid_search_logistic_regression(df=df, col='vec_links_normalized', col_is_tfidf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking combined features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [00:22<03:48,  2.51s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from analysis_functions import *\n",
    "\n",
    "def train_logistic_regression(df, col, col_is_tfidf=False):\n",
    "    '''\n",
    "    model, tf_X_train, tf_X_test, tf_y_train, tf_y_test, X, tf_vocab = train_logistic_regression(df, 'tokenized')\n",
    "    tf_y_pred = model.predict(tf_X_test)\n",
    "    '''\n",
    "    if not col_is_tfidf:\n",
    "        X_tfidf, tf_vocab, X = tfidf_transformation(df, col)\n",
    "    else:\n",
    "        X_tfidf = np.array(df[col].tolist())\n",
    "        tf_vocab = None\n",
    "\n",
    "    y = df['y'].to_numpy()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y)\n",
    "    \n",
    "    # Model optimized for tf-idf combined\n",
    "    model = LogisticRegression(C=3792.690191, penalty='l1', solver='liblinear').fit(X_train, y_train)\n",
    "    # model = LogisticRegression(C=100000, penalty='l1', solver='liblinear').fit(X_train, y_train)\n",
    "\n",
    "    if not col_is_tfidf:\n",
    "        return model, X_train, X_test, y_train, y_test, X.toarray(), tf_vocab\n",
    "    else:\n",
    "        return model, X_train, X_test, y_train, y_test, X_tfidf, tf_vocab\n",
    "\n",
    "def evaluate_logistic_regression(model, tf_X_test, tf_y_test, tf_vocab, PRINT=True):\n",
    "    \n",
    "    tf_y_pred = model.predict(tf_X_test)\n",
    "    \n",
    "    f1 = f1_score(tf_y_test, tf_y_pred)\n",
    "    precision = precision_score(tf_y_test, tf_y_pred)\n",
    "    recall = recall_score(tf_y_test, tf_y_pred)\n",
    "    accuracy = accuracy_score(tf_y_test, tf_y_pred)\n",
    "    \n",
    "    baseline = np.sum(tf_y_test) / tf_y_test.shape[0]\n",
    "    if baseline < 0.5:\n",
    "        baseline = 1 - baseline\n",
    "\n",
    "    if PRINT:\n",
    "        print('\\n\\nlogistic regression classifier\\n-------------\\naccuracy: {:.4} %\\nbaseline: {:.4} %'.format(accuracy*100, np.max(baseline)*100))\n",
    "        print('\\nf1:         {:.4f}\\nprecision:  {:.4f}\\nrecall:     {:.4f}\\n\\n'.format(f1, precision, recall))\n",
    "\n",
    "        if tf_vocab is not None:\n",
    "            print('size of vocab: {}\\n'.format(len(tf_vocab)))\n",
    "\n",
    "            fake_idx = model.coef_.argsort()[0][-20:][::-1]\n",
    "            real_idx = model.coef_.argsort()[0][:20][::-1]\n",
    "\n",
    "            real_words = []\n",
    "            fake_words = []\n",
    "\n",
    "            for i in range(len(real_idx)):\n",
    "                real_words.append(tf_vocab[real_idx[i]])\n",
    "                fake_words.append(tf_vocab[fake_idx[i]])\n",
    "\n",
    "            top_words = pd.DataFrame(list(zip(real_words, fake_words)), columns=['top \"real\" words', 'top \"fake\" words'])\n",
    "            print(top_words)\n",
    "\n",
    "    return accuracy, f1, precision, recall\n",
    "\n",
    "def confidence_interval_logistic_regression(num_iterations, df, col='combined_features', col_is_tfidf=False, PRINT=False):\n",
    "    '''\n",
    "    Trains logistic regression classifiers on unmodified TF-IDF vectors from corpus\n",
    "    '''\n",
    "    print('Checking combined features')\n",
    "    results = np.zeros(shape=(4, num_iterations))\n",
    "    for j in tqdm(range(num_iterations)):\n",
    "        model, _, X_test, _, y_test, _, tf_vocab = train_logistic_regression(df, col, col_is_tfidf)\n",
    "        accuracy, f1, precision, recall = evaluate_logistic_regression(model, X_test, y_test, tf_vocab, PRINT=PRINT)\n",
    "        results[0, j] = accuracy\n",
    "        results[1, j] = f1\n",
    "        results[2, j] = precision\n",
    "        results[3, j] = recall\n",
    "\n",
    "    results_df = populate_results_dataframe(results)\n",
    "    print(results_df)\n",
    "    # boxplot_results(results)\n",
    "\n",
    "confidence_interval_logistic_regression(100, df, col='combined_text_500', col_is_tfidf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_interval_linear_SVM(100, df, col='combined_text_500', col_is_tfidf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "best estimator:\n",
      "Pipeline(steps=[('classifier',\n",
      "                 SVC(C=10000.0, decision_function_shape='ovo', kernel='linear',\n",
      "                     probability=True, tol=0.0001))])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def grid_search_linear_SVM(df, col, param_grid = None, col_is_tfidf=False):\n",
    "    if not param_grid:\n",
    "        # param_grid = [\n",
    "                     # {'classifier' : [SVC()],\n",
    "                      # 'classifier__kernel' : ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "                      # 'classifier__C' : np.logspace(-4, 4, 20),\n",
    "                      # 'classifier__probability' : [True, False],\n",
    "                      # 'classifier__tol' : [1e-4, 1e-3, 1e-2],\n",
    "                      # 'classifier__decision_function_shape' : ['ovo', 'ovr']}\n",
    "                      # ]\n",
    "        param_grid = [\n",
    "                     {'classifier' : [SVC()],\n",
    "                      'classifier__kernel' : ['linear'],\n",
    "                      'classifier__C' : np.logspace(-4, 4, 20),\n",
    "                      'classifier__probability' : [True],\n",
    "                      'classifier__tol' : [1e-4],\n",
    "                      'classifier__decision_function_shape' : ['ovo']}\n",
    "                      ]\n",
    "\n",
    "    if not col_is_tfidf:\n",
    "        X_tfidf, tf_vocab, X = tfidf_transformation(df, col)\n",
    "    else:\n",
    "        X_tfidf = np.array(df[col].tolist())\n",
    "        tf_vocab = None\n",
    "        \n",
    "    y = df['y'].to_numpy()\n",
    "    X_train, _, y_train, _ = train_test_split(X_tfidf, y)\n",
    "\n",
    "    pipe = Pipeline([('classifier' , SVC())])\n",
    "\n",
    "    model = GridSearchCV(pipe, param_grid=param_grid, cv=5, verbose=10, n_jobs=-1).fit(X_train, y_train)\n",
    "    best_model = model.fit(X_train, y_train)\n",
    "\n",
    "    print('best estimator:')\n",
    "    print(best_model.best_estimator_)\n",
    "    return\n",
    "\n",
    "grid_search_linear_SVM(df=df, col='vec_links_normalized', col_is_tfidf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [2:03:58<00:00, 74.39s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      metric  lower_bound      mean  upper_bound\n",
      "0   accuracy     0.946882  0.951027     0.955171\n",
      "1         f1     0.909614  0.916681     0.923747\n",
      "2  precision     0.968079  0.974043     0.980007\n",
      "3     recall     0.852784  0.865827     0.878870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from analysis_functions import *\n",
    "\n",
    "def train_linear_SVM(df, col, col_is_tfidf=False):\n",
    "    if not col_is_tfidf:\n",
    "        X_tfidf, tf_vocab, X = tfidf_transformation(df, col)\n",
    "    else:\n",
    "        X_tfidf = np.array(df[col].tolist())\n",
    "        tf_vocab = None\n",
    "        \n",
    "    y = df['y'].to_numpy()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y)\n",
    "\n",
    "    if not col_is_tfidf:\n",
    "        # optimized for tf-idf combined\n",
    "        # model = SVC(C = 11.288378916846883, decision_function_shape='ovo', probability=True, tol=1e-4).fit(X_train.toarray(), y_train)\n",
    "\n",
    "        # optimized for tf-idf, prepended, combined\n",
    "        model = SVC(C=0.00026366508987303583, decision_function_shape='ovo',\n",
    "                        kernel='linear', probability=True, tol=0.0001).fit(X_train.toarray(), y_train)\n",
    "    else:\n",
    "        model = SVC(C=10000, decision_function_shape='ovo', probability=True, tol=0.0001).fit(X_train, y_train)\n",
    "\n",
    "    if not col_is_tfidf:\n",
    "        return model, X_train, X_test, y_train, y_test, X.toarray(), tf_vocab\n",
    "    else:\n",
    "        return model, X_train, X_test, y_train, y_test, X_tfidf, tf_vocab\n",
    "\n",
    "def evaluate_linear_SVM(model, X_test, y_test, col_is_tfidf = False):\n",
    "    \n",
    "    if not col_is_tfidf:\n",
    "        y_pred = model.predict(X_test.toarray())\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "\n",
    "    return accuracy, f1, precision, recall\n",
    "\n",
    "def confidence_interval_linear_SVM(num_iterations, df, col, col_is_tfidf=False):\n",
    "    results = np.zeros(shape=(4, num_iterations))\n",
    "    for j in tqdm(range(num_iterations)):\n",
    "        model, _, X_test, _, y_test, _, vocab = train_linear_SVM(df, col=col, col_is_tfidf=col_is_tfidf)\n",
    "        accuracy, f1, precision, recall = evaluate_linear_SVM(model, X_test, y_test, col_is_tfidf=col_is_tfidf)\n",
    "        results[0, j] = accuracy\n",
    "        results[1, j] = f1\n",
    "        results[2, j] = precision\n",
    "        results[3, j] = recall\n",
    "    \n",
    "    results_df = populate_results_dataframe(results)\n",
    "    print(results_df)\n",
    "    # boxplot_results(results)\n",
    "\n",
    "    return results\n",
    "\n",
    "results = confidence_interval_linear_SVM(100, df, col='vec_links_normalized', col_is_tfidf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "confidence_interval_logistic_regression() got an unexpected keyword argument 'col_is_tfidf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d3e42380e407>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'combined_text_500'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent_tfidf_500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_tfidf_500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfidence_interval_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'combined_text_500'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_is_tfidf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: confidence_interval_logistic_regression() got an unexpected keyword argument 'col_is_tfidf'"
     ]
    }
   ],
   "source": [
    "df['combined_text_500'] = df.apply(lambda row: np.concatenate((row.content_tfidf_500, row.meta_tfidf_500), axis=0),axis=1)\n",
    "results = confidence_interval_logistic_regression(100, df, col='combined_text_500', col_is_tfidf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [29:53<00:00, 17.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      metric  lower_bound      mean  upper_bound\n",
      "0   accuracy     0.939071  0.944203     0.949335\n",
      "1         f1     0.895618  0.904871     0.914125\n",
      "2  precision     0.952966  0.961369     0.969773\n",
      "3     recall     0.840039  0.854789     0.869539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = confidence_interval_linear_SVM(100, df, col='meta_tfidf_500', col_is_tfidf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [08:39<00:00,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      metric  lower_bound      mean  upper_bound\n",
      "0   accuracy     0.921989  0.927360     0.932730\n",
      "1         f1     0.865866  0.874753     0.883641\n",
      "2  precision     0.938092  0.948280     0.958468\n",
      "3     recall     0.797867  0.811978     0.826088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = confidence_interval_linear_SVM(100, df, col='meta_tfidf_100', col_is_tfidf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, combine the vectors into a single feature, the order will be [content, meta, hyperlinking]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_features(df):\n",
    "    df['combined_features'] = df.apply(lambda row: np.concatenate((row.content_tfidf, row.meta_tfidf, row.vectorized_links), axis=0),axis=1)\n",
    "    return df\n",
    "\n",
    "df = combine_features(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df\n",
    "with open('full_df.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate the relationship between classifier performance and number of tokens retained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis_functions import confidence_interval_logistic_regression\n",
    "\n",
    "results = confidence_interval_logistic_regression(100, df, 'content_tfidf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1131efc7635b497546d7e8fbc76ad9d1f9d5d5d7857bcde935d6feea39d08984"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
