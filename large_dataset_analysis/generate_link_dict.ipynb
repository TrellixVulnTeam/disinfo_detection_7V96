{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After the initial breadth first search, we take the list of scraped links and create a dictionary. This will go back to the openwpm scraper to get HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAKE FAILED\n",
      "[2, 8, 18, 21, 39, 41, 61, 76, 122, 141, 148, 170, 186, 190, 191, 192, 199, 212, 230, 243, 246, 248, 249, 271, 276, 279, 333, 345, 371, 379, 387, 391, 418, 423, 434, 449, 462, 474, 476, 487, 500, 517, 527, 543, 549, 551, 552, 572, 605, 610, 645, 667, 699, 703, 714, 728, 731, 752, 776, 778, 781, 782, 783, 799, 829, 831, 857, 883, 890, 907, 909, 913, 918, 950, 954, 955, 956, 968, 969, 1002, 1031, 1036, 1090, 1100, 1107, 1108, 1126, 1128, 1136, 1171, 1176, 1186, 1188, 1214, 1215, 1222, 1223, 1224, 1245, 1257]\n",
      "['http://ahtribune.com', 'http://alphanewsmn.com', 'http://americanfreedombybarbara.com', 'http://americanmarxism.net', 'http://anguillesousroche.com', 'http://anonews.co', 'http://babybiden.com', 'http://biggovernment.com', 'http://cartoonstock.com', 'http://chicksontheright.com', 'http://christophercantwell.com', 'http://collective-evolution.com', 'http://conservativefighters.com', 'http://conservativehq.com', 'http://conservativemedia.com', 'http://conservativeminute.com', 'http://conservativetribune.com', 'http://counterpunch.com', 'http://dailyexpose.co.uk', 'http://dashamerica.com', 'http://davidharrisjr.com', 'http://dbdailyupdate.com', 'http://dcalert.com', 'http://dimplus.nl', 'http://dividist.com', 'http://dmlnews.com', 'http://explainlife.com', 'http://fashthenation.com', 'http://fourwinds10.net', 'http://freedomoutpost.com', 'http://french.presstv.com', 'http://fuhrernet.org', 'http://godfatherpolitics.com', 'http://gopworld.com', 'http://hangthebankers.com', 'http://hermancain.com', 'http://hydraulicspneumatics.com', 'http://imowired.com', 'http://importer.bitchute.com', 'http://intellihub.com', 'http://israelvideonetwork.com', 'http://jonesreport.com', 'http://kiwifarms.net', 'http://leftexposed.org', 'http://liberalpropagandaexposed.com', 'http://libertyalliance.org', 'http://libertyalliancellc.com', 'http://louderwithcrowder.com', 'http://merinews.com', 'http://michellemalkin.com', 'http://naturallydaily.com', 'http://newsinsideout.com', 'http://oann.news', 'http://occasion-to-be.com', 'http://onenewsnow.com', 'http://pajamasmedia.com', 'http://pamelageller.com', 'http://phiquyenchinh.org', 'http://pravdareport.com', 'http://presstv.com', 'http://principia-scientific.org', 'http://prisonplanet.com', 'http://prisonplanet.tv', 'http://readersupportednews.org', 'http://rightdiagnosis.com', 'http://rightwing.news', 'http://sagaciousnewsnetwork.com', 'http://shariawatch.org.uk', 'http://shoryuken.com', 'http://spartareport.com', 'http://spectator.us', 'http://sptnkne.ws', 'http://status.bitchute.com', 'http://stopthestealgeorgia.com', 'http://stopthestealmovement.us', 'http://stopthestealnews.com', 'http://stopthestealnews.org', 'http://stopthestealupdates.com', 'http://stopthestealupdates.org', 'http://tbdailynews.com', 'http://thecrier.net', 'http://thedonald.win', 'http://theresurgent.com', 'http://thetruthaboutcovid.net', 'http://thewashingtonpundit.com', 'http://thewashingtonsentinel.com', 'http://tr.news', 'http://trendingviews.co', 'http://trumporcruz.com', 'http://vaccinesrevealed.com', 'http://vernoncoleman.com', 'http://voiceofeurope.com', 'http://volokh.com', 'http://westernjournalism.com', 'http://westmonster.com', 'http://whitegirlbleedalot.com', 'http://whiteresister.com', 'http://whowhatwhy.com', 'http://ww1.gatewaypundit.com', 'http://zionica.com']\n",
      "FAKE UNSAVED\n",
      "[218, 219, 220, 251, 264, 392, 463, 475, 516, 524, 570, 574, 589, 590, 591, 707, 708, 709, 710, 726, 757, 845, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 952, 953, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 1115, 1180]\n",
      "['http://crushthesteal.com', 'http://crushthesteal.net', 'http://crushthesteal.org', 'http://dcdirtylaundry.com', 'http://deutsche-stimme.de', 'http://fury.news', 'http://iceagenow.info', 'http://impeccablehealth.com', 'http://joebidensmelledme.com', 'http://katehon.com', 'http://longroom.com', 'http://lovewale.com', 'http://marchtostopthesteal.com', 'http://marchtostopthesteal.net', 'http://marchtostopthesteal.org', 'http://officialproudboys.com', 'http://officialstopthesteal.com', 'http://officialstopthesteal.net', 'http://officialstopthesteal.org', 'http://pac.leadershipinstitute.org', 'http://pmnightlynews.com', 'http://rose-bear-uae.website', 'http://stopthestealaction.com', 'http://stopthestealaction.net', 'http://stopthestealaction.org', 'http://stopthestealcampaign.com', 'http://stopthestealcampaign.net', 'http://stopthestealcampaign.org', 'http://stopthestealcoalition.com', 'http://stopthestealcoalition.net', 'http://stopthestealcoalition.org', 'http://stopthestealcommittee.com', 'http://stopthestealcommittee.net', 'http://stopthestealcommittee.org', 'http://stopthestealconference.com', 'http://stopthestealconference.net', 'http://stopthestealconference.org', 'http://stopthestealconvention.com', 'http://stopthestealconvention.net', 'http://stopthestealconvention.org', 'http://stopthestealdc.com', 'http://stopthestealevent.com', 'http://stopthestealevents.com', 'http://stopthestealfilm.com', 'http://stopthestealga.com', 'http://stopthestealga.org', 'http://stopthestealmovement.net', 'http://stopthestealmovement.org', 'http://stopthestealofficial.com', 'http://stopthestealpac.com', 'http://stopthestealpac.org', 'http://stopthestealprotest.com', 'http://stopthestealprotests.com', 'http://stopthestealprotests.net', 'http://stopthestealprotests.org', 'http://stopthestealrallies.com', 'http://stopthestealsaturday.com', 'http://stopthestealtownhall.com', 'http://stopthestealtownhalls.com', 'http://thoushallnotsteal.org', 'http://viewty.com']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "from preprocessing_functions import *\n",
    "\n",
    "# # REAL SITES\n",
    "# with open('500_real_list.pkl', 'rb') as f:\n",
    "    # src_list_real = pickle.load(f)\n",
    "# f.close()\n",
    "# outfile_real = '500_real_df.pkl'\n",
    "# path_to_src_lists_real = Path.cwd().parent.parent / 'datadir' / 'newsguard' / 'real_links' / 'sources'\n",
    "\n",
    "# failed_real, unsaved_real = check_for_unscraped_sites(path_to_src_lists_real, src_list_real)\n",
    "\n",
    "# # FAKE SITES\n",
    "# with open('500_fake_list.pkl', 'rb') as f:\n",
    "    # src_list_fake = pickle.load(f)\n",
    "# f.close()\n",
    "# outfile_fake = '500_fake_df.pkl'\n",
    "# path_to_src_lists_fake = Path.cwd().parent.parent / 'datadir' / 'newsguard' / 'fake_links' / 'sources'\n",
    "\n",
    "\n",
    "# failed_fake, unsaved_fake = check_for_unscraped_sites(path_to_src_lists_fake, src_list_fake)\n",
    "\n",
    "# REAL SITES\n",
    "with open('GDI_list.pkl', 'rb') as f:\n",
    "    gdi_list = pickle.load(f)\n",
    "f.close()\n",
    "outfile_real = 'GDI_df.pkl'\n",
    "path_to_src_lists = Path.cwd().parent.parent / 'datadir' / 'GDI' / 'links' / 'sources'\n",
    "\n",
    "failed_gdi, unsaved_gdi = check_for_unscraped_sites(path_to_src_lists, gdi_list)\n",
    "evaluate_failed_sites(failed_gdi, unsaved_gdi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When lots of sites are failed and unsaved, modify the CSV so that new sites can be put in their place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSVs\n",
    "\n",
    "real_csv = pd.read_csv(Path.cwd().parent / 'fake_real_CSVS' / '500_real.csv')\n",
    "fake_csv = pd.read_csv(Path.cwd().parent / 'fake_real_CSVS' / '500_fake.csv')\n",
    "\n",
    "# Change these with each rescrape\n",
    "unsaved_col_name = 'unsaved_3'\n",
    "failed_col_name = 'failed_3'\n",
    "\n",
    "def modify_csv_for_failed_sites(df, unsaved, failed, unsaved_col_name, failed_col_name):\n",
    "    '''\n",
    "    Takes df (read from a csv) and modifies it so that new sites can be manually added after a scrape fails\n",
    "\n",
    "    ~~~~ ARGUMENTS ~~~~\n",
    "    df : DataFrame\n",
    "        - read from csv, for example 500_real.csv\n",
    "    unsaved : dictionary\n",
    "        - Output of check_for_unscraped_sites()\n",
    "    failed: dictionary\n",
    "        - Output of check_for_unscraped_sites()\n",
    "\n",
    "    ~~~~ RETURNS ~~~~\n",
    "    modified_df : DataFrame\n",
    "        - Can be saved as a CSV, formatted for easier manual site addition\n",
    "    '''\n",
    "    # Insert columns if they're not there already\n",
    "    if failed_col_name not in df.columns:\n",
    "        df[failed_col_name] = ''\n",
    "    if unsaved_col_name not in df.columns:\n",
    "        df[unsaved_col_name] = ''\n",
    "    \n",
    "    for k,v in unsaved.items():\n",
    "        df.at[k, unsaved_col_name] = df.loc[[k]]['source'].values[0]\n",
    "        df.at[k, 'source'] = ''\n",
    "    for k,v in failed.items():\n",
    "        df.at[k, failed_col_name] = df.loc[[k]]['source'].values[0]\n",
    "        df.at[k, 'source'] = ''\n",
    "    \n",
    "    return df\n",
    "\n",
    "real_csv_modified = modify_csv_for_failed_sites(real_csv, unsaved_real, failed_real, unsaved_col_name, failed_col_name)\n",
    "fake_csv_modified = modify_csv_for_failed_sites(fake_csv, unsaved_fake, failed_fake, unsaved_col_name, failed_col_name)\n",
    "\n",
    "# Save new csvs\n",
    "fake_csv_modified.to_csv('500_fake.csv', index=False)\n",
    "real_csv_modified.to_csv('500_real.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After the CSV is modified, generate a new custom list of sites to scrape.\n",
    "this can be implemented in bfls_<fake or real>.py as new_idx and new_list. Which should be accessed in the command loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAKE RESCRAPE\n",
      "[352]\n",
      "['http://thenorthstar.com']\n",
      "REAL RESCRAPE\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "fake_csv = pd.read_csv(Path.cwd().parent / 'fake_real_CSVs' / '500_fake.csv')\n",
    "real_csv = pd.read_csv(Path.cwd().parent / 'fake_real_CSVs' / '500_real.csv')\n",
    "\n",
    "def get_rescrape_lists(df, unsaved_col_name, failed_col_name):\n",
    "    '''\n",
    "    Given a dataframe read from a CSV with new sites to scrape, this function will generate lists to put in the scraping script\n",
    "    '''\n",
    "    failed_idx = df[failed_col_name].notnull().values\n",
    "    unsaved_idx =  df[unsaved_col_name].notnull().values\n",
    "    f_idx = [i for i, x in enumerate(failed_idx) if x]\n",
    "    u_idx = [i for i, x in enumerate(unsaved_idx) if x]\n",
    "    \n",
    "    modified_idx = f_idx + u_idx\n",
    "    modified_idx.sort()\n",
    "\n",
    "    urls = []\n",
    "    # Get new list of urls\n",
    "    for idx in modified_idx:\n",
    "        urls.append(df.at[idx, 'source'])\n",
    "\n",
    "    urls = ['http://' + url for url in urls]\n",
    " \n",
    "    return modified_idx, urls\n",
    "\n",
    "rescrape_indices_fake, rescrape_urls_fake = get_rescrape_lists(fake_csv, unsaved_col_name, failed_col_name)\n",
    "print('FAKE RESCRAPE')\n",
    "print(rescrape_indices_fake)\n",
    "print(rescrape_urls_fake)\n",
    "\n",
    "rescrape_indices_real, rescrape_urls_real = get_rescrape_lists(real_csv, unsaved_col_name, failed_col_name)\n",
    "print('REAL RESCRAPE')\n",
    "print(rescrape_indices_real)\n",
    "print(rescrape_urls_real)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "\n",
    "\n",
    "df_real = generate_link_dataframe(path_to_src_lists_real, src_list_real, outfile_real)\n",
    "df_fake = generate_link_dataframe(path_to_src_lists_fake, src_list_fake, outfile_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For GDI\n",
    "\n",
    "df_GDI = generate_link_dataframe(path_to_src_lists, gdi_list, outfile_real, failed_sites=failed_gdi, unsaved_sites=unsaved_gdi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site</th>\n",
       "      <th>num_links</th>\n",
       "      <th>link_dict</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://ae911truth.org</td>\n",
       "      <td>3605</td>\n",
       "      <td>{0: 'https://www.ae911truth.org/', 1: 'https:/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://afa.net</td>\n",
       "      <td>108</td>\n",
       "      <td>{0: 'https://afa.net/', 1: 'https://afa.net/th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://aim.org</td>\n",
       "      <td>522</td>\n",
       "      <td>{0: 'https://www.aim.org/', 1: 'https://www.ai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://aim4truth.org</td>\n",
       "      <td>97</td>\n",
       "      <td>{0: 'https://aim4truth.org/', 1: 'https://aim4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://alipac.us</td>\n",
       "      <td>166</td>\n",
       "      <td>{0: 'http://www.alipac.us/content/', 1: 'https...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        site num_links  \\\n",
       "index                                    \n",
       "0      http://ae911truth.org      3605   \n",
       "1             http://afa.net       108   \n",
       "3             http://aim.org       522   \n",
       "4       http://aim4truth.org        97   \n",
       "5           http://alipac.us       166   \n",
       "\n",
       "                                               link_dict  \n",
       "index                                                     \n",
       "0      {0: 'https://www.ae911truth.org/', 1: 'https:/...  \n",
       "1      {0: 'https://afa.net/', 1: 'https://afa.net/th...  \n",
       "3      {0: 'https://www.aim.org/', 1: 'https://www.ai...  \n",
       "4      {0: 'https://aim4truth.org/', 1: 'https://aim4...  \n",
       "5      {0: 'http://www.alipac.us/content/', 1: 'https...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_GDI.head()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1131efc7635b497546d7e8fbc76ad9d1f9d5d5d7857bcde935d6feea39d08984"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
